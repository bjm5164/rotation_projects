{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cloudvolume import CloudVolume\n",
    "import json\n",
    "from annotationframeworkclient import FrameworkClient\n",
    "import nglui\n",
    "from concurrent import futures\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to the annotation engine:\n",
    "#### 1. Get a JSON state\n",
    "#### 2. Upload neurons or synapses\n",
    "---\n",
    "### You need two tokens for this to work currently, since you cannont generate a cloudvolume object using the api token.  \n",
    "##### Token links can be obtained from the annotation framework wiki\n",
    "- These tokens are currently stored in ~/cloudvolume/secrets/chunkedgraph-secret.json, you should put them here too since this folder will be used by other things. \n",
    "- For now, you can either copy your token directly, or set up a json file that stores them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup credentials.\n",
    "   - Run this only once. It will set up your cloudvolume folder so that you can have easy access to the authentication tokens as well as the segmentation links. Use the links provided to get authentication tokens. You need an api and a dev token for the notebook to run completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for setting up your cloudvolume folder. \n",
    "\n",
    "def setup_credentials(tokens,segmentations,overwrite=False):\n",
    "    ''' Setup the api keys and segmentation links in ~/cloudvolume. \n",
    "    Args:\n",
    "        tokens: dict, hex string api tokens. The other modules currently use two. 'api' and 'dev' and both are currently necessary. api\n",
    "                is used for generating an annotation framework client, and dev is used for creating a cloudvolume object.\n",
    "        segmentations: dict, segmentation paths and respective resolutions. Format is {'segmentation_name':{'url':'path_to_segmentation','resolution':'[x,y,z]'}}' '''\n",
    "\n",
    "\n",
    "    BASE = Path.home() / 'cloudvolume'\n",
    "\n",
    "\n",
    "    if Path.exists(BASE / 'secrets'):\n",
    "        if Path.exists(BASE / 'secrets' / 'chunkedgraph-secret.json') and overwrite is False:\n",
    "            print('credentials exist')\n",
    "        else:\n",
    "            with open(Path.home() / 'cloudvolume' / 'secrets'/'chunkedgraph-secret.json',mode='w') as f:\n",
    "                json.dump(tokens,f)\n",
    "        print('credentials created')\n",
    "\n",
    "    else: \n",
    "        Path.mkdir(BASE / 'secrets', parents=True)\n",
    "        with open(Path.home() / 'cloudvolume' / 'secrets'/'chunkedgraph-secret.json',mode='w') as f:\n",
    "                json.dump(tokens,f)\n",
    "        print('credentials created')\n",
    "\n",
    "    if not Path.exists(BASE / 'segmentations.json'):\n",
    "        with open(BASE / 'segmentations.json',mode='w') as f:\n",
    "            json.dump(segmentations,f)\n",
    "        print('setup complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_token = None\n",
    "api_token = None\n",
    "tokens = {'dev':dev_token,'api':api_token}\n",
    "\n",
    "version_1_segmentation = None\n",
    "segmentations = {'Dynamic_V1':version_1_segmentation}\n",
    "\n",
    "setup_credentials(tokens,segmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have this set up, comment it out and copy the token directly. \n",
    "with open(Path.home() / 'cloudvolume' / 'secrets'/'chunkedgraph-secret.json') as f:\n",
    "        tokens = json.load(f)\n",
    "\n",
    "\n",
    "# This token is for accessing the cloud volume for the V1 chunkedgraph. If copying directly, pass it as a string like 'asdfghjkl1234='\n",
    "dev_token = tokens['dev']\n",
    "# This token is for interacting with the annotation framework.\n",
    "auth_token = tokens['api']\n",
    "\n",
    "datastack_name = 'vnc_v0' # from https://api.zetta.ai/wclee/info/\n",
    "\n",
    "client = FrameworkClient(\n",
    "    datastack_name,\n",
    "    server_address = \"https://api.zetta.ai/wclee\",\n",
    "    auth_token = auth_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the JSON state from a NG instance containing an annotation layer.\n",
    "- JSON states are the primary means of managing things.\n",
    "- When working in neuroglancer, to save the state of your workspace, press ctrl-shift-j. This will change the url with the /xxxxx being the JSON state ID. These states are stored in the JSON state service of the annotation framework. They are NOT EASY to look up, so here is a method for managing them.\n",
    "- Define a state manager to keep track of our states. Save the state dataframe to the ~/.cloudvolume. We will put this in a module later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager: \n",
    "    \n",
    "    ''' Class for keeping track of JSON states.'''\n",
    "    def __init__(self,\n",
    "                 filename=None,\n",
    "                 token=None):\n",
    "        \n",
    "        self.directory = Path.home() / 'cloudvolume'\n",
    "        if filename is None:\n",
    "            self.filename = self.directory / 'json_states.csv'\n",
    "        \n",
    "        self.__initialize()\n",
    "        \n",
    "        \n",
    "    def __initialize(self):\n",
    "        # Check if the database exists, if not create a new one.\n",
    "        fileEmpty =  os.path.exists(self.filename)\n",
    "        if not fileEmpty:\n",
    "            df = pd.DataFrame(columns=['state_id','description'])\n",
    "            df.to_csv(self.filename,index=False)\n",
    "        self.get_database()\n",
    "        print(self.df) \n",
    "    \n",
    "    def get_database(self):\n",
    "        # Read database. \n",
    "        self.df  = pd.read_csv(self.filename)\n",
    "        \n",
    " \n",
    "    def add_state(self, state_id, description=None):\n",
    "        \n",
    "        filename = self.filename\n",
    "        df = pd.DataFrame([{'state_id':state_id,'description':description}])\n",
    "        df.to_csv(filename, mode='a', header=False,index=False, encoding = 'utf-8')\n",
    "        self.get_database()\n",
    "        return('state added')\n",
    "\n",
    "    \n",
    "    def get_url(self,state):\n",
    "        return('https://neuromancer-seung-import.appspot.com/?json_url=https://api.zetta.ai/json/' + str(state))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the state ID you want, or add a state with `sm.add_state()`\n",
    "#### Next, get the JSON state with `sm.get_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              state_id                                       description\n",
      "0   261002187096550146                                    MNs,10Bs,Claws\n",
      "1   838783366227209333  Slow Tibia Flexor in V1 chunkedgraph and V2 flat\n",
      "2   838783366227209333  Slow Tibia Flexor in V1 chunkedgraph and V2 flat\n",
      "3   585521373111957130              Comparison of V1 and V2 chunkedgraph\n",
      "4   758492429940665723                                     9A_T1 neurons\n",
      "5   299984539931822600                                              PMNs\n",
      "6   336757619423424673                          81A07 synapse annotation\n",
      "7   769530493626036401                                     Chen2020 data\n",
      "8   837109578740814693                           9a_beta-club annotation\n",
      "9   975616946463965161                 Slow Tibia Flexor synapses-Mollie\n",
      "10       3294823948324                                synapse anntoation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://neuromancer-seung-import.appspot.com/?json_url=https://api.zetta.ai/json/837109578740814693'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.state.get_state_json(837109578740814693)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at available layers\n",
    "- If you had annotation layers in your state, marking either synapses or cells, they will be shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nglui.parser.layer_names(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some methods for formatting schemas. \n",
    "##### We are going to use two schemas for now. \n",
    "1. The synapse schema for synapses.\n",
    "    - To generate synapse entries, use an annotation layer that used line annotations with pt1 being the presynapse and pt2 being the postsynspse. \n",
    "2. The bound_tag schema for marking cells. \n",
    "    - To generate cell entreis, use an annotation layer that used point annotations, preferably on the soma (but not in the nucleus unless you merged the nucleus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soma_table_entries(state,layer_name = 'cells'):\n",
    "    ''' Generate entries for a soma table using the bound_tag schema \n",
    "    Args:\n",
    "    state: json, json state from get_json_state\n",
    "    layer_name: name of layer containing soma coords\n",
    "    ### Change this to points later. This is just for database copying because I used a stupid annotation\n",
    "    \n",
    "    Returns:\n",
    "    entries: list, list of dict entries for the bound_tag schema'''\n",
    "    \n",
    "    cell_layer = nglui.parser.get_layer(state,layer_name)\n",
    "    entries = []\n",
    "    for i in cell_layer['annotations']:\n",
    "        entry = {'tag': i['description'],\n",
    "                 'pt': {'position': i['point']}}\n",
    "        entries.append(entry)\n",
    "    return(entries)\n",
    "        \n",
    "\n",
    "def upload_cells(client,cell_entries,table_name,description=None):\n",
    "    ''' Upload cell entries to a soma dable using the bound_tag schema'\n",
    "    Args:\n",
    "    cell_entries: list, list of dicts from soma_table_entries\n",
    "    table_name: str, table name to create. If it exists, it will populate the existing one. \n",
    "    description: str, description to supply if creating a new table. It will fail if you don't provide one. \n",
    "    \n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='bound_tag',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in cell_entries:\n",
    "        try:\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[i])\n",
    "        except:\n",
    "            print('Fail',entry)\n",
    "\n",
    "\n",
    "\n",
    "def get_synapses(state,synapse_layer='synapses'):\n",
    "    ''' Get the synapse coordinates from a json state and return pre,post,center.\n",
    "    Args:\n",
    "        state: json,  json state from get_state_json \n",
    "        synapse_layer: str, name of layer containing synapses in the json state. Default is 'synapses'\n",
    "    Returns:\n",
    "        pre_pt,post_pt,ctr_pt: list, lists of coordinates for synapses.'''\n",
    "    \n",
    "    \n",
    "    syns = nglui.parser.line_annotations(state,synapse_layer)\n",
    "    if np.shape(syns)[0] != 2:\n",
    "        raise Exception( print('Incorrectly formatted synapse annotation. Requires two lists: Presynapse coordinates, Postsynapse coordinates'))\n",
    "           \n",
    "    else:\n",
    "        pre_pt = syns[0]\n",
    "        post_pt = syns[1]\n",
    "        ctr_pt = (np.array(pre_pt) + np.array(post_pt)) / 2\n",
    "    return(pre_pt,post_pt,np.ndarray.tolist(ctr_pt))\n",
    "\n",
    "\n",
    "def format_synapse(pre,post,ctr):\n",
    "    ''' Format a synapse for upload to a synapse table\n",
    "    Args:\n",
    "           pre: list, mip0 xyz coordinates to presynapse\n",
    "           post: list, mip0 xyz coordinates to postsynapse\n",
    "           ctr: list, mip0 xyz coordinates to center point\n",
    "    Returns:\n",
    "           data: formatted dict for a synapse table annotation upload\n",
    "    '''\n",
    "    data = {\n",
    "    \"type\": \"synapse\",\n",
    "    'pre_pt': {'position': pre},\n",
    "    'post_pt': {'position': post},\n",
    "    'ctr_pt': {'position': ctr}\n",
    "\n",
    "}\n",
    "    return(data)\n",
    "\n",
    "def upload_synapses(client,synapse_coordinates,table_name,description=None):\n",
    "    ''' Add synapses to a synapse table.\n",
    "    Args: \n",
    "        synapse_coordinates: list, list of shape [3,n,3]. Dim 0 is [pre,post,ctr], Dim 1 is the entry, Dim 2 is [x,y,z]. Output of get_synapses\n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "    '''\n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='synapse',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in range(np.shape(synapse_coordinates)[1]):\n",
    "        try:\n",
    "            entry = format_synapse(synapse_coordinates[0][i],synapse_coordinates[1][i],synapse_coordinates[2][i])\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[entry])\n",
    "        except:\n",
    "            print('Fail',entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a table from an annotation layer\n",
    "##### There currently are not checks on uploading duplicate data, so do not go crazy with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = '9A-beta_synapses'\n",
    "table_name = 'brandon_synapses'\n",
    "description = 'Club - 9a synapses'\n",
    "\n",
    "# FOR CELLS\n",
    "#entries = soma_table_entries(state,layer_name=layer_name)\n",
    "#upload_cells(entries,table_name,description=description)\n",
    "\n",
    "# FOR SYNAPSES\n",
    "entries = get_synapses(state,synapse_layer=layer_name)\n",
    "upload_synapses(client,entries,table_name,description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some methods for working around the materialization engine.\n",
    "    1. Download an annotation table\n",
    "    2. Look up the root_ids associated with the points\n",
    "    3. Generate a dataframe that looks like the microns format (post materialization?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_annotation_table(client,table_name,ids=range(100000)):\n",
    "    entries = client.annotation.get_annotation(table_name,ids)\n",
    "    annotation_table = pd.DataFrame(entries)\n",
    "    return(annotation_table)\n",
    "\n",
    "\n",
    "def get_sv_pairs(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=False,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    pre_ids = seg_from_pt(annotation_table.pre_pt_position,cv)\n",
    "    post_ids = seg_from_pt(annotation_table.post_pt_position,cv)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(list(zip(pre_ids,post_ids)))\n",
    "\n",
    "\n",
    "\n",
    "def remove_entries(table,sv_pairs,inplace=True):\n",
    "    \n",
    "    og_set = set() \n",
    "    d_idx = [] \n",
    "    for idx, val in enumerate(sv_pairs): \n",
    "        if val not in og_set: \n",
    "            og_set.add(val)          \n",
    "        else: \n",
    "            d_idx.append(idx)\n",
    "            \n",
    "    table = table.drop(d_idx,axis=0,inplace=inplace)\n",
    "    return(table)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seg_from_pt(pts,vol,image_res=np.array([4.3,4.3,45]),max_workers=4):\n",
    "    ''' Get segment ID at a point. Default volume is the static segmentation layer for now. \n",
    "    Args:\n",
    "        pts (list): list of 3-element np.arrays of MIP0 coordinates\n",
    "        vol_url (str): cloud volume url\n",
    "    Returns:\n",
    "        list, segment_ID at specified point '''\n",
    "    \n",
    "    \n",
    "    seg_mip = vol.scale['resolution']\n",
    "    res = seg_mip / image_res\n",
    "\n",
    "    pts_scaled = [pt // res for pt in pts]\n",
    "    results = []\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        point_futures = [ex.submit(lambda pt,vol: vol[list(pt)][0][0][0][0], k,vol) for k in pts_scaled]\n",
    "        \n",
    "        for f in futures.as_completed(point_futures):\n",
    "            results=[f.result() for f in point_futures]\n",
    "       \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_soma_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "\n",
    "    soma_table = pd.DataFrame(columns=['name','cell_type',\n",
    "                                       'pt_position','pt_root_id',\n",
    "                                       'soma_x_nm','soma_y_nm','soma_z_nm',\n",
    "                                       'found'])\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    seg_ids = seg_from_pt(annotation_table.pt_position,cv)\n",
    "    \n",
    "    soma_table.name = annotation_table.tag\n",
    "    soma_table.pt_position = annotation_table.pt_position\n",
    "    soma_table.pt_root_id = seg_ids\n",
    "    soma_table.soma_x_nm = np.array([i[0] for i in annotation_table.pt_position]) * resolution[0]\n",
    "    soma_table.soma_y_nm = np.array([i[1] for i in annotation_table.pt_position]) * resolution[1]\n",
    "    soma_table.soma_z_nm = np.array([i[2] for i in annotation_table.pt_position]) * resolution[2]\n",
    "    \n",
    "    return(soma_table)\n",
    "\n",
    "\n",
    "\n",
    "def generate_synapse_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "     \n",
    "    \n",
    "    synapse_table = pd.DataFrame(columns=['id','pre_root_id','post_root_id',\n",
    "                                      'cleft_vx','ctr_pt_x_nm','ctr_pt_y_nm','ctr_pt_z_nm',\n",
    "                                      'pre_pos_x_vx','pre_pos_y_vx','pre_pos_z_vx',\n",
    "                                      'ctr_pos_x_vx','ctr_pos_y_vx','ctr_pos_z_vx',\n",
    "                                      'post_pos_x_vx','post_pos_y_vx','post_pos_z_vx'])\n",
    "\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    pre_ids = seg_from_pt(annotation_table.pre_pt_position,cv)\n",
    "    post_ids = seg_from_pt(annotation_table.post_pt_position,cv)\n",
    "    \n",
    "    synapse_table.pre_root_id = pre_ids\n",
    "    synapse_table.post_root_id = post_ids\n",
    "    \n",
    "    # TODO: This in not a stupid way. \n",
    "    synapse_table.ctr_pt_x_nm = np.array([i[0] for i in annotation_table.ctr_pt_position]) * resolution[0]\n",
    "    synapse_table.ctr_pt_y_nm = np.array([i[1] for i in annotation_table.ctr_pt_position]) * resolution[1]\n",
    "    synapse_table.ctr_pt_z_nm = np.array([i[2] for i in annotation_table.ctr_pt_position]) * resolution[2]\n",
    "    \n",
    "    synapse_table.pre_pos_x_vx = np.array([i[0] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_y_vx = np.array([i[1] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_z_vx = np.array([i[2] for i in annotation_table.pre_pt_position]) \n",
    "    \n",
    "    synapse_table.post_pos_x_vx = np.array([i[0] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_y_vx = np.array([i[1] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_z_vx = np.array([i[2] for i in annotation_table.post_pt_position]) \n",
    "    \n",
    "    return(synapse_table)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download tables\n",
    "- This will download tables from the annotation engine to then look up segment IDs and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_synapse_table', 'test_synapse_table_2', 'test_T1MN_soma_table', 'T1MN_somas', '10B_somas', 'claw_neurons', '81A07_Synapses', 'test_table', 'club_synapses', 'claw_neuron_synapses', 'test', 'synapses', 'MB_synapses', 'Ltm synapse', 'Mollie_synapses']\n"
     ]
    }
   ],
   "source": [
    "print(client.annotation.get_tables())\n",
    "#cell_table = download_annotation_table('test_T1MN_soma_table')\n",
    "syn_table = download_annotation_table(client,'synapses',ids=range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sv_pairs = get_sv_pairs(syn_table,token=dev_token)\n",
    "remove_entries(syn_table,sv_pairs,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert annotation tables materialized(?) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soma_table = generate_soma_table(cell_table,token=dev_token)\n",
    "\n",
    "synapse_table = generate_synapse_table(syn_table,token=dev_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_table['pre_root_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid</th>\n",
       "      <th>size</th>\n",
       "      <th>pre_pt_position</th>\n",
       "      <th>ctr_pt_position</th>\n",
       "      <th>post_pt_position</th>\n",
       "      <th>deleted</th>\n",
       "      <th>superceded_id</th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[62709, 101566, 2865]</td>\n",
       "      <td>[62733, 101592, 2865]</td>\n",
       "      <td>[62756, 101617, 2865]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-04 21:26:09.742881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[64010, 100303, 2730]</td>\n",
       "      <td>[64025, 100330, 2730]</td>\n",
       "      <td>[64040, 100357, 2730]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-04 21:26:10.327422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[64135, 100500, 2656]</td>\n",
       "      <td>[64105, 100470, 2656]</td>\n",
       "      <td>[64074, 100441, 2656]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-02-04 21:26:10.596379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[63331, 100283, 2658]</td>\n",
       "      <td>[63357, 100299, 2658]</td>\n",
       "      <td>[63383, 100315, 2658]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-02-04 21:26:10.718089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[64049, 103688, 3150]</td>\n",
       "      <td>[64034, 103658, 3150]</td>\n",
       "      <td>[64020, 103627, 3150]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-02-04 21:26:10.841057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[60331, 102697, 2094]</td>\n",
       "      <td>[60334, 102714, 2094]</td>\n",
       "      <td>[60338, 102732, 2094]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>860</td>\n",
       "      <td>2021-02-04 21:28:28.732154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[59151, 97981, 2908]</td>\n",
       "      <td>[59144, 97995, 2908]</td>\n",
       "      <td>[59137, 98009, 2908]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>861</td>\n",
       "      <td>2021-02-04 21:28:28.851785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[63790, 100572, 2773]</td>\n",
       "      <td>[63783, 100552, 2773]</td>\n",
       "      <td>[63776, 100533, 2773]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>862</td>\n",
       "      <td>2021-02-04 21:28:28.974861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[60459, 108121, 3232]</td>\n",
       "      <td>[60454, 108131, 3231]</td>\n",
       "      <td>[60450, 108141, 3230]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>863</td>\n",
       "      <td>2021-02-04 21:28:29.094990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[59944, 108988, 3198]</td>\n",
       "      <td>[59961, 108966, 3198]</td>\n",
       "      <td>[59977, 108943, 3198]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>864</td>\n",
       "      <td>2021-02-04 21:28:29.252250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>861 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valid  size        pre_pt_position        ctr_pt_position  \\\n",
       "0     True  None  [62709, 101566, 2865]  [62733, 101592, 2865]   \n",
       "1     True  None  [64010, 100303, 2730]  [64025, 100330, 2730]   \n",
       "2     True  None  [64135, 100500, 2656]  [64105, 100470, 2656]   \n",
       "3     True  None  [63331, 100283, 2658]  [63357, 100299, 2658]   \n",
       "4     True  None  [64049, 103688, 3150]  [64034, 103658, 3150]   \n",
       "..     ...   ...                    ...                    ...   \n",
       "859   True  None  [60331, 102697, 2094]  [60334, 102714, 2094]   \n",
       "860   True  None   [59151, 97981, 2908]   [59144, 97995, 2908]   \n",
       "861   True  None  [63790, 100572, 2773]  [63783, 100552, 2773]   \n",
       "862   True  None  [60459, 108121, 3232]  [60454, 108131, 3231]   \n",
       "863   True  None  [59944, 108988, 3198]  [59961, 108966, 3198]   \n",
       "\n",
       "          post_pt_position deleted superceded_id   id  \\\n",
       "0    [62756, 101617, 2865]    None          None    1   \n",
       "1    [64040, 100357, 2730]    None          None    2   \n",
       "2    [64074, 100441, 2656]    None          None    3   \n",
       "3    [63383, 100315, 2658]    None          None    4   \n",
       "4    [64020, 103627, 3150]    None          None    5   \n",
       "..                     ...     ...           ...  ...   \n",
       "859  [60338, 102732, 2094]    None          None  860   \n",
       "860   [59137, 98009, 2908]    None          None  861   \n",
       "861  [63776, 100533, 2773]    None          None  862   \n",
       "862  [60450, 108141, 3230]    None          None  863   \n",
       "863  [59977, 108943, 3198]    None          None  864   \n",
       "\n",
       "                        created  \n",
       "0    2021-02-04 21:26:09.742881  \n",
       "1    2021-02-04 21:26:10.327422  \n",
       "2    2021-02-04 21:26:10.596379  \n",
       "3    2021-02-04 21:26:10.718089  \n",
       "4    2021-02-04 21:26:10.841057  \n",
       "..                          ...  \n",
       "859  2021-02-04 21:28:28.732154  \n",
       "860  2021-02-04 21:28:28.851785  \n",
       "861  2021-02-04 21:28:28.974861  \n",
       "862  2021-02-04 21:28:29.094990  \n",
       "863  2021-02-04 21:28:29.252250  \n",
       "\n",
       "[861 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a4674b40504e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics_analysis",
   "language": "python",
   "name": "connectomics_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
